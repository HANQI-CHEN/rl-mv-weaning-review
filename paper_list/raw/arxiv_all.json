{
  "queries": [
    "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean",
    "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:extubat",
    "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:\"spontaneous breathing trial\"",
    "all:\"reinforcement learning\" AND all:ventilator AND all:wean",
    "all:\"reinforcement learning\" AND all:ventilator AND all:extubat"
  ],
  "fallback_query": "all:\"reinforcement learning\" AND (all:\"mechanical ventilation\" OR all:ventilator)",
  "max_results_per_page": 200,
  "pages_fetched": 2,
  "totals_reported": [
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean",
      "total": 2
    },
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:extubat",
      "total": 0
    },
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:\"spontaneous breathing trial\"",
      "total": 0
    },
    {
      "query": "all:\"reinforcement learning\" AND all:ventilator AND all:wean",
      "total": 2
    },
    {
      "query": "all:\"reinforcement learning\" AND all:ventilator AND all:extubat",
      "total": 0
    }
  ],
  "pages": [
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean",
      "start": 0,
      "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3A%22reinforcement%20learning%22%20AND%20all%3A%22mechanical%20ventilation%22%20AND%20all%3Awean%26id_list%3D%26start%3D0%26max_results%3D200\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean&amp;id_list=&amp;start=0&amp;max_results=200</title>\n  <id>http://arxiv.org/api/2d6gfCKm6JNISMEdJ5OALT0KhYg</id>\n  <updated>2025-10-08T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">200</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1704.06300v1</id>\n    <updated>2017-04-20T18:53:51Z</updated>\n    <published>2017-04-20T18:53:51Z</published>\n    <title>A Reinforcement Learning Approach to Weaning of Mechanical Ventilation\n  in Intensive Care Units</title>\n    <summary>  The management of invasive mechanical ventilation, and the regulation of\nsedation and analgesia during ventilation, constitutes a major part of the care\nof patients admitted to intensive care units. Both prolonged dependence on\nmechanical ventilation and premature extubation are associated with increased\nrisk of complications and higher hospital costs, but clinical opinion on the\nbest protocol for weaning patients off of a ventilator varies. This work aims\nto develop a decision support tool that uses available patient information to\npredict time-to-extubation readiness and to recommend a personalized regime of\nsedation dosage and ventilator support. To this end, we use off-policy\nreinforcement learning algorithms to determine the best action at a given\npatient state from sub-optimal historical ICU data. We compare treatment\npolicies from fitted Q-iteration with extremely randomized trees and with\nfeedforward neural networks, and demonstrate that the policies learnt show\npromise in recommending weaning protocols with improved outcomes, in terms of\nminimizing rates of reintubation and regulating physiological stability.\n</summary>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Li-Fang Cheng</name>\n    </author>\n    <author>\n      <name>Corey Chivers</name>\n    </author>\n    <author>\n      <name>Michael Draugelis</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1704.06300v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.06300v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1905.13167v1</id>\n    <updated>2019-05-30T16:51:49Z</updated>\n    <published>2019-05-30T16:51:49Z</published>\n    <title>Defining Admissible Rewards for High Confidence Policy Evaluation</title>\n    <summary>  A key impediment to reinforcement learning (RL) in real applications with\nlimited, batch data is defining a reward function that reflects what we\nimplicitly know about reasonable behaviour for a task and allows for robust\noff-policy evaluation. In this work, we develop a method to identify an\nadmissible set of reward functions for policies that (a) do not diverge too far\nfrom past behaviour, and (b) can be evaluated with high confidence, given only\na collection of past trajectories. Together, these ensure that we propose\npolicies that we trust to be implemented in high-risk settings. We demonstrate\nour approach to reward design on synthetic domains as well as in a critical\ncare context, for a reward that consolidates clinical objectives to learn a\npolicy for weaning patients from mechanical ventilation.\n</summary>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n    <author>\n      <name>Finale Doshi-Velez</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1905.13167v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.13167v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"
    },
    {
      "query": "all:\"reinforcement learning\" AND all:ventilator AND all:wean",
      "start": 0,
      "xml": "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<feed xmlns=\"http://www.w3.org/2005/Atom\">\n  <link href=\"http://arxiv.org/api/query?search_query%3Dall%3A%22reinforcement%20learning%22%20AND%20all%3Aventilator%20AND%20all%3Awean%26id_list%3D%26start%3D0%26max_results%3D200\" rel=\"self\" type=\"application/atom+xml\"/>\n  <title type=\"html\">ArXiv Query: search_query=all:\"reinforcement learning\" AND all:ventilator AND all:wean&amp;id_list=&amp;start=0&amp;max_results=200</title>\n  <id>http://arxiv.org/api/g7neE9c0XjUGxAaquByicTltxb0</id>\n  <updated>2025-10-08T00:00:00-04:00</updated>\n  <opensearch:totalResults xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">2</opensearch:totalResults>\n  <opensearch:startIndex xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">0</opensearch:startIndex>\n  <opensearch:itemsPerPage xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\">200</opensearch:itemsPerPage>\n  <entry>\n    <id>http://arxiv.org/abs/1704.06300v1</id>\n    <updated>2017-04-20T18:53:51Z</updated>\n    <published>2017-04-20T18:53:51Z</published>\n    <title>A Reinforcement Learning Approach to Weaning of Mechanical Ventilation\n  in Intensive Care Units</title>\n    <summary>  The management of invasive mechanical ventilation, and the regulation of\nsedation and analgesia during ventilation, constitutes a major part of the care\nof patients admitted to intensive care units. Both prolonged dependence on\nmechanical ventilation and premature extubation are associated with increased\nrisk of complications and higher hospital costs, but clinical opinion on the\nbest protocol for weaning patients off of a ventilator varies. This work aims\nto develop a decision support tool that uses available patient information to\npredict time-to-extubation readiness and to recommend a personalized regime of\nsedation dosage and ventilator support. To this end, we use off-policy\nreinforcement learning algorithms to determine the best action at a given\npatient state from sub-optimal historical ICU data. We compare treatment\npolicies from fitted Q-iteration with extremely randomized trees and with\nfeedforward neural networks, and demonstrate that the policies learnt show\npromise in recommending weaning protocols with improved outcomes, in terms of\nminimizing rates of reintubation and regulating physiological stability.\n</summary>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Li-Fang Cheng</name>\n    </author>\n    <author>\n      <name>Corey Chivers</name>\n    </author>\n    <author>\n      <name>Michael Draugelis</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1704.06300v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1704.06300v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1905.13167v1</id>\n    <updated>2019-05-30T16:51:49Z</updated>\n    <published>2019-05-30T16:51:49Z</published>\n    <title>Defining Admissible Rewards for High Confidence Policy Evaluation</title>\n    <summary>  A key impediment to reinforcement learning (RL) in real applications with\nlimited, batch data is defining a reward function that reflects what we\nimplicitly know about reasonable behaviour for a task and allows for robust\noff-policy evaluation. In this work, we develop a method to identify an\nadmissible set of reward functions for policies that (a) do not diverge too far\nfrom past behaviour, and (b) can be evaluated with high confidence, given only\na collection of past trajectories. Together, these ensure that we propose\npolicies that we trust to be implemented in high-risk settings. We demonstrate\nour approach to reward design on synthetic domains as well as in a critical\ncare context, for a reward that consolidates clinical objectives to learn a\npolicy for weaning patients from mechanical ventilation.\n</summary>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n    <author>\n      <name>Finale Doshi-Velez</name>\n    </author>\n    <link href=\"http://arxiv.org/abs/1905.13167v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link title=\"pdf\" href=\"http://arxiv.org/pdf/1905.13167v1\" rel=\"related\" type=\"application/pdf\"/>\n    <arxiv:primary_category xmlns:arxiv=\"http://arxiv.org/schemas/atom\" term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n  </entry>\n</feed>\n"
    }
  ]
}