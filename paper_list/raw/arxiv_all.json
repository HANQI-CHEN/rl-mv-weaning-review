{
  "queries": [
    "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean",
    "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:extubat",
    "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:\"spontaneous breathing trial\"",
    "all:\"reinforcement learning\" AND all:ventilator AND all:wean",
    "all:\"reinforcement learning\" AND all:ventilator AND all:extubat"
  ],
  "fallback_query": "all:\"reinforcement learning\" AND (all:\"mechanical ventilation\" OR all:ventilator)",
  "max_results_per_page": 200,
  "pages_fetched": 2,
  "totals_reported": [
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean",
      "total": 2
    },
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:extubat",
      "total": 0
    },
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:\"spontaneous breathing trial\"",
      "total": 0
    },
    {
      "query": "all:\"reinforcement learning\" AND all:ventilator AND all:wean",
      "total": 2
    },
    {
      "query": "all:\"reinforcement learning\" AND all:ventilator AND all:extubat",
      "total": 0
    }
  ],
  "pages": [
    {
      "query": "all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean",
      "start": 0,
      "xml": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/2d6gfCKm6JNISMEdJ5OALT0KhYg</id>\n  <title>arXiv Query: search_query=all:\"reinforcement learning\" AND all:\"mechanical ventilation\" AND all:wean&amp;id_list=&amp;start=0&amp;max_results=200</title>\n  <updated>2025-11-30T01:20:24Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:%22reinforcement+learning%22+AND+(all:%22mechanical+ventilation%22+AND+all:wean)&amp;start=0&amp;max_results=200&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>200</opensearch:itemsPerPage>\n  <opensearch:totalResults>2</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/1704.06300v1</id>\n    <title>A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units</title>\n    <updated>2017-04-20T18:53:51Z</updated>\n    <link href=\"https://arxiv.org/abs/1704.06300v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1704.06300v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Q-iteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2017-04-20T18:53:51Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Li-Fang Cheng</name>\n    </author>\n    <author>\n      <name>Corey Chivers</name>\n    </author>\n    <author>\n      <name>Michael Draugelis</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1905.13167v1</id>\n    <title>Defining Admissible Rewards for High Confidence Policy Evaluation</title>\n    <updated>2019-05-30T16:51:49Z</updated>\n    <link href=\"https://arxiv.org/abs/1905.13167v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1905.13167v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>A key impediment to reinforcement learning (RL) in real applications with limited, batch data is defining a reward function that reflects what we implicitly know about reasonable behaviour for a task and allows for robust off-policy evaluation. In this work, we develop a method to identify an admissible set of reward functions for policies that (a) do not diverge too far from past behaviour, and (b) can be evaluated with high confidence, given only a collection of past trajectories. Together, these ensure that we propose policies that we trust to be implemented in high-risk settings. We demonstrate our approach to reward design on synthetic domains as well as in a critical care context, for a reward that consolidates clinical objectives to learn a policy for weaning patients from mechanical ventilation.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2019-05-30T16:51:49Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n    <author>\n      <name>Finale Doshi-Velez</name>\n    </author>\n  </entry>\n</feed>\n"
    },
    {
      "query": "all:\"reinforcement learning\" AND all:ventilator AND all:wean",
      "start": 0,
      "xml": "<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/g7neE9c0XjUGxAaquByicTltxb0</id>\n  <title>arXiv Query: search_query=all:\"reinforcement learning\" AND all:ventilator AND all:wean&amp;id_list=&amp;start=0&amp;max_results=200</title>\n  <updated>2025-11-30T01:20:30Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:%22reinforcement+learning%22+AND+(all:ventilator+AND+all:wean)&amp;start=0&amp;max_results=200&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>200</opensearch:itemsPerPage>\n  <opensearch:totalResults>2</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/1704.06300v1</id>\n    <title>A Reinforcement Learning Approach to Weaning of Mechanical Ventilation in Intensive Care Units</title>\n    <updated>2017-04-20T18:53:51Z</updated>\n    <link href=\"https://arxiv.org/abs/1704.06300v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1704.06300v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The management of invasive mechanical ventilation, and the regulation of sedation and analgesia during ventilation, constitutes a major part of the care of patients admitted to intensive care units. Both prolonged dependence on mechanical ventilation and premature extubation are associated with increased risk of complications and higher hospital costs, but clinical opinion on the best protocol for weaning patients off of a ventilator varies. This work aims to develop a decision support tool that uses available patient information to predict time-to-extubation readiness and to recommend a personalized regime of sedation dosage and ventilator support. To this end, we use off-policy reinforcement learning algorithms to determine the best action at a given patient state from sub-optimal historical ICU data. We compare treatment policies from fitted Q-iteration with extremely randomized trees and with feedforward neural networks, and demonstrate that the policies learnt show promise in recommending weaning protocols with improved outcomes, in terms of minimizing rates of reintubation and regulating physiological stability.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2017-04-20T18:53:51Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Li-Fang Cheng</name>\n    </author>\n    <author>\n      <name>Corey Chivers</name>\n    </author>\n    <author>\n      <name>Michael Draugelis</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1905.13167v1</id>\n    <title>Defining Admissible Rewards for High Confidence Policy Evaluation</title>\n    <updated>2019-05-30T16:51:49Z</updated>\n    <link href=\"https://arxiv.org/abs/1905.13167v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1905.13167v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>A key impediment to reinforcement learning (RL) in real applications with limited, batch data is defining a reward function that reflects what we implicitly know about reasonable behaviour for a task and allows for robust off-policy evaluation. In this work, we develop a method to identify an admissible set of reward functions for policies that (a) do not diverge too far from past behaviour, and (b) can be evaluated with high confidence, given only a collection of past trajectories. Together, these ensure that we propose policies that we trust to be implemented in high-risk settings. We demonstrate our approach to reward design on synthetic domains as well as in a critical care context, for a reward that consolidates clinical objectives to learn a policy for weaning patients from mechanical ventilation.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"stat.ML\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2019-05-30T16:51:49Z</published>\n    <arxiv:primary_category term=\"cs.LG\"/>\n    <author>\n      <name>Niranjani Prasad</name>\n    </author>\n    <author>\n      <name>Barbara E Engelhardt</name>\n    </author>\n    <author>\n      <name>Finale Doshi-Velez</name>\n    </author>\n  </entry>\n</feed>\n"
    }
  ]
}